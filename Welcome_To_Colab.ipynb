{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rey2512/Model/blob/main/Welcome_To_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4hYJvPUiDiH6",
        "outputId": "b418c871-a865-405f-e345-e9be564daa5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from timm import create_model\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "BASE_PATH = \"/content/drive/MyDrive/1DeepFake/splitted_data\"\n",
        "TRAIN_REAL_PATH = os.path.join(BASE_PATH, \"real/train\")\n",
        "TRAIN_FAKE_PATH = os.path.join(BASE_PATH, \"fake/train\")\n",
        "VAL_REAL_PATH   = os.path.join(BASE_PATH, \"real/val\")\n",
        "VAL_FAKE_PATH   = os.path.join(BASE_PATH, \"fake/val\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "class DeepFakeDataset(Dataset):\n",
        "    def __init__(self, file_list, labels, transform=None):\n",
        "        self.file_list = file_list\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.file_list[idx]\n",
        "        cap = cv2.VideoCapture(path)\n",
        "        ret, frame = cap.read()\n",
        "        cap.release()\n",
        "        if not ret:\n",
        "            frame = np.zeros((299, 299, 3), dtype=np.uint8)\n",
        "        frame = cv2.resize(frame, (299, 299))\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        if self.transform:\n",
        "            frame = self.transform(frame)\n",
        "        label = self.labels[idx]\n",
        "        return frame, label\n"
      ],
      "metadata": {
        "id": "PP-Cdm7ND4Ue",
        "outputId": "81ae9100-2c32-44c4-bef2-98b761820217",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: efficientnet_pytorch in /usr/local/lib/python3.11/dist-packages (0.7.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from efficientnet_pytorch) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->efficientnet_pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->efficientnet_pytorch) (3.0.2)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (1.17.0)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.11/dist-packages (1.21.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.4)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.onnx\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import timm\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "# =======================\n",
        "# ðŸ“Œ Define Constants\n",
        "# =======================\n",
        "DATASET_PATH = \"/content/drive/MyDrive/splitted_data\"\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/best_deepfake_detector2.pth\"\n",
        "BATCH_SIZE = 12  # Increased batch size\n",
        "EPOCHS = 30\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "PATIENCE = 3  # Early stopping patience\n",
        "\n",
        "# =======================\n",
        "# ðŸ“Œ Data Augmentation & Dataset\n",
        "# =======================\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "class DeepFakeDataset(Dataset):\n",
        "    def __init__(self, folder, label, transform=None, frames_per_video=30):\n",
        "        self.folder = folder\n",
        "        self.label = label\n",
        "        self.transform = transform\n",
        "        self.frames_per_video = frames_per_video\n",
        "        self.video_files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith(\".mp4\")]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = self.video_files[idx]\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frames = []\n",
        "\n",
        "        while len(frames) < self.frames_per_video and cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame = cv2.resize(frame, (224, 224))\n",
        "            frame = self.transform(frame)\n",
        "            frames.append(frame)\n",
        "        cap.release()\n",
        "\n",
        "        while len(frames) < self.frames_per_video:\n",
        "            frames.append(torch.zeros(3, 224, 224))\n",
        "\n",
        "        return torch.stack(frames), torch.tensor(self.label, dtype=torch.long)\n",
        "\n",
        "# =======================\n",
        "# ðŸ“Œ Load Dataset\n",
        "# =======================\n",
        "train_real = DeepFakeDataset(os.path.join(DATASET_PATH, \"real/train\"), label=0, transform=transform)\n",
        "train_fake = DeepFakeDataset(os.path.join(DATASET_PATH, \"fake/train\"), label=1, transform=transform)\n",
        "val_real = DeepFakeDataset(os.path.join(DATASET_PATH, \"real/val\"), label=0, transform=transform)\n",
        "val_fake = DeepFakeDataset(os.path.join(DATASET_PATH, \"fake/val\"), label=1, transform=transform)\n",
        "\n",
        "test_real = DeepFakeDataset(os.path.join(DATASET_PATH, \"real/test\"), label=0, transform=transform)\n",
        "test_fake = DeepFakeDataset(os.path.join(DATASET_PATH, \"fake/test\"), label=1, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_real + train_fake, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_real + val_fake, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_real + test_fake, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# =======================\n",
        "# ðŸ“Œ Define Model\n",
        "# =======================\n",
        "class DeepFakeDetector(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepFakeDetector, self).__init__()\n",
        "        self.feature_extractor = timm.create_model(\"efficientnet_lite0\", pretrained=True)\n",
        "        self.feature_extractor.classifier = nn.Identity()\n",
        "        self.lstm = nn.LSTM(input_size=1280, hidden_size=256, num_layers=1, batch_first=True, dropout=0.3)\n",
        "        self.fc = nn.Linear(256, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C, H, W = x.shape\n",
        "        x = x.view(B * T, C, H, W)\n",
        "        features = self.feature_extractor(x)\n",
        "        features = features.view(B, T, -1)\n",
        "        lstm_out, _ = self.lstm(features)\n",
        "        output = self.fc(lstm_out[:, -1, :])\n",
        "        return output\n",
        "\n",
        "# =======================\n",
        "# ðŸ“Œ Train Model & Track Performance\n",
        "# =======================\n",
        "model = DeepFakeDetector().to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "train_losses, val_losses, val_accuracies = [], [], []\n",
        "best_val_loss = float('inf')\n",
        "stopping_rounds = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_preds, train_labels = [], [] # Initialize lists to store predictions and labels\n",
        "    for videos, labels in train_loader:\n",
        "        videos, labels = videos.to(DEVICE), labels.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model(videos)\n",
        "            loss = criterion(outputs, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Accumulate predictions and labels for calculating accuracy\n",
        "        train_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "        train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    train_acc = accuracy_score(train_labels, train_preds) # Calculate training accuracy\n",
        "    train_losses.append(train_loss / len(train_loader))\n",
        "\n",
        "    model.eval()\n",
        "    val_loss, val_preds, val_labels = 0, [], []\n",
        "    with torch.no_grad():\n",
        "        for videos, labels in val_loader:\n",
        "            videos, labels = videos.to(DEVICE), labels.to(DEVICE)\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(videos)\n",
        "                loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            val_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "    val_acc = accuracy_score(val_labels, val_preds)\n",
        "    val_losses.append(val_loss / len(val_loader))\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}%, \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc*100:.2f}%\")\n",
        "    scheduler.step()\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "        stopping_rounds = 0\n",
        "    else:\n",
        "        stopping_rounds += 1\n",
        "        if stopping_rounds >= PATIENCE:\n",
        "            break\n",
        "\n",
        "# =======================\n",
        "# ðŸ“Œ Plot Loss & Accuracy Graphs\n",
        "# =======================\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Val Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss over Epochs')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(val_accuracies, label='Val Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Validation Accuracy over Epochs')\n",
        "plt.show()\n",
        "\n",
        "# =======================\n",
        "# ðŸ“Œ Final Test Evaluation\n",
        "# =======================\n",
        "model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for videos, labels in test_loader:\n",
        "        videos, labels = videos.to(DEVICE), labels.to(DEVICE)\n",
        "        outputs = model(videos)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "print(classification_report(all_labels, all_preds, digits=4))\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ATFKkHpyD6PY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import timm\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# =======================\n",
        "# ðŸ“Œ Define Constants\n",
        "# =======================\n",
        "MODEL_PATH = \"/content/drive/MyDrive/1DeepFake/best_deepfake_detector2.pth\"  # Update path if needed\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "FRAME_SIZE = (224, 224)\n",
        "FRAMES_PER_VIDEO = 30\n",
        "\n",
        "# =======================\n",
        "# ðŸ“Œ Define Model (Same as Training)\n",
        "# =======================\n",
        "class DeepFakeDetector(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepFakeDetector, self).__init__()  # Corrected the typo\n",
        "        self.feature_extractor = timm.create_model(\"efficientnet_lite0\", pretrained=False)\n",
        "        self.feature_extractor.classifier = nn.Identity()\n",
        "        # Checking output size of EfficientNet-Lite0\n",
        "        feature_output_size = self.feature_extractor.num_features\n",
        "        self.lstm = nn.LSTM(input_size=feature_output_size, hidden_size=256, num_layers=1, batch_first=True, dropout=0.3)\n",
        "        self.fc = nn.Linear(256, 2)  # Output 2 classes: REAL, FAKE\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C, H, W = x.shape\n",
        "        x = x.view(B * T, C, H, W)  # Flatten time dimension\n",
        "        features = self.feature_extractor(x)  # Extract features\n",
        "        features = features.view(B, T, -1)  # Shape (B, T, features)\n",
        "        lstm_out, _ = self.lstm(features)  # LSTM output\n",
        "        output = self.fc(lstm_out[:, -1, :])  # Take the last time step\n",
        "        return output\n",
        "\n",
        "# Load model\n",
        "model = DeepFakeDetector().to(DEVICE)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "model.eval()\n",
        "\n",
        "# =======================\n",
        "# ðŸ“Œ Preprocessing Function\n",
        "# =======================\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize(FRAME_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "def preprocess_video(video_path, frames_per_video=FRAMES_PER_VIDEO):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "\n",
        "    while len(frames) < frames_per_video and cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frame = transform(frame)  # Apply transformations\n",
        "        frames.append(frame)\n",
        "    cap.release()\n",
        "\n",
        "    while len(frames) < frames_per_video:\n",
        "        frames.append(torch.zeros(3, *FRAME_SIZE))  # Padding if not enough frames\n",
        "\n",
        "    return torch.stack(frames).unsqueeze(0)  # Shape: (1, T, C, H, W)\n",
        "\n",
        "# =======================\n",
        "# ðŸ“Œ Inference Function\n",
        "# =======================\n",
        "def predict(video_path):\n",
        "    video_tensor = preprocess_video(video_path).to(DEVICE)  # Preprocess video\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(video_tensor)  # Get model output\n",
        "        probabilities = F.softmax(output, dim=1)  # Convert logits to probabilities\n",
        "        fake_prob = probabilities[0, 1].item() * 100  # Get probability of fake class\n",
        "\n",
        "    label = \"FAKE\" if fake_prob > 50 else \"REAL\"  # Classify as fake or real\n",
        "    print(f\"Prediction: {label} | Fake Probability: {fake_prob:.2f}%\")\n",
        "\n",
        "# =======================\n",
        "# ðŸ“Œ Run Inference\n",
        "# =======================\n",
        "video_path = \"/content/drive/MyDrive/Ncsc /WhatsApp Video 2025-04-04 at 22.24.05_f291f0c6.mp4\"  # Replace with your test video path\n",
        "predict(video_path)\n"
      ],
      "metadata": {
        "id": "GuRj_8DhD98w",
        "outputId": "6c85bc68-fde9-458a-adec-cad091ba68f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: FAKE | Fake Probability: 51.73%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}